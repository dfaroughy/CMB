#...General experiment parameters
experiment:
  run_name: jetclass_generation_CMB_final
  workdir: /home/df630/CMB/results/runs

#...Data parameters

data:
  target:
    name: JetClass
    params:
      min_num_particles: 0
      max_num_particles: 128
    train:
      path:
        - /home/df630/CMB/data/JetClass/TTBar_000.root
        - /home/df630/CMB/data/JetClass/TTBar_001.root
        - /home/df630/CMB/data/JetClass/TTBar_002.root
    test:
      path:
        - /home/df630/CMB/data/JetClass/TTBar_003.root

  source: 
    name: GaussNoise
    params:
      scale: 1.0
      cat_probs: [0.4135, 0.1093, 0.4647, 0.0076, 0.0049]
      min_num_particles: 0
      max_num_particles: 128
    train:
      path: null
    test:
      path: null
      
  dim:
    features:
      continuous: 3             # pt_rel, eta_rel, phi_phi
      discrete: 1               # num of transition rates
    context:
      continuous: 0
      discrete: 0      
  
  vocab:
    size: 
      features: 8 
      context: 0  

  preprocess:
    continuous: standardize
    discrete: states
    
#...Model parameters

model: 
  name: HybridEPiC
  num_blocks: 10

  dim:
    embed:
      time: 16
      features:
        continuous: 16
        discrete: 16
      context:
        continuous: 0
        discrete: 0
    hidden: 
      local: 128
      glob: 16

  embed_type:
    time: SinusoidalPositionalEncoding
    features: 
      continuous: Linear
      discrete: Embedding
    context:
      continuous: null
      discrete: null

  skip_connection: true
  dropout: 0.1
  activation: SELU

#...Dynamics parameters

dynamics: 
  name: ConditionalMarkovBridge
  loss_weight: 1.0
  loss_weight_fn: null

  continuous:
    process: FlowMatching
    sigma: 0.0001

  discrete:
    process: TelegraphProcess
    gamma: 0.125

#...Pipeline generation parameters

pipeline:
  method: EulerLeapingSolver
  num_timesteps: 1000
  time_eps: 0.0001

#...Training parameters

train:
  device: cuda:0
  multi_gpu: false
  batch_size: 1024
  data_split_frac: [0.9, 0.1, 0.0]  # train / val / test
  epochs: 1000
  early_stopping: null
  min_epochs: null
  print_epochs: 10
  num_workers: 0
  pin_memory: false

  optimizer:
    name: AdamW
    params:
      lr: 0.001
      weight_decay: 5.0e-5
      betas: [0.9, 0.999]
      eps: 1.e-8
      amsgrad: false
    gradient_clip: 1.0

  scheduler: 
    name: CosineAnnealingLR
    params: 
      T_max: 1000
      eta_min: 1.0e-6
      last_epoch: -1


