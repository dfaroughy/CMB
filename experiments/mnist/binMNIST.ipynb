{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple\n",
    "\n",
    "@dataclass\n",
    "class Configs:\n",
    "    #...general params:\n",
    "    WORKDIR : str = './'   \n",
    "    DEVICE : str = 'cuda:3'\n",
    "    MULTI_GPU : bool = False\n",
    "\n",
    "    #...data params:\n",
    "    DATA : str = None\n",
    "    DATA_TARGET : str = 'mnist'\n",
    "    DATA_SOURCE : str = 'noise'\n",
    "    DIM_INPUT : int = None\n",
    "    DIM_CONTEXT : int = 0\n",
    "    VOCAB_SIZE : int = 2\n",
    "    INPUT_SHAPE : Tuple[float] = field(default_factory = lambda : (1, 28, 28))\n",
    "    FLAT_IMAGE : bool = True\n",
    "\n",
    "    # #...model params:\n",
    "    MODEL : str = 'StateClassifier'\n",
    "    DIM_HIDDEN : int = 128\n",
    "    DIM_TIME_EMB : int = 32\n",
    "    DIM_STATE_EMB : int = 8\n",
    "    NUM_LAYERS : int = 3\n",
    "    DROPOUT : float = 0.1\n",
    "    ACTIVATION : str = 'ReLU'\n",
    "    TIME_EMBEDDING_TYPE : str = 'sinusoidal'\n",
    "\n",
    "    #...training params:\n",
    "    BATCH_SIZE : int = 128\n",
    "    DATA_SPLIT_FRACS : List[float] = field(default_factory = lambda : [0.83334, 0.16667, 0.0])  # train / val / test \n",
    "    EPOCHS: int = 20\n",
    "    EARLY_STOPPING : int = None\n",
    "    MIN_EPOCHS : int = None \n",
    "    PRINT_EPOCHS : int = None   \n",
    "    NUM_WORKERS : int = 0\n",
    "    PIN_MEMORY: bool = False\n",
    "\n",
    "    #...cjb params:\n",
    "    DYNAMICS : str = 'ConditionalJumpBridge'\n",
    "    GAMMA: float = 0.1\n",
    "\n",
    "    #...optimization & scheduler params:\n",
    "    OPTIMIZER: str = 'Adam'\n",
    "    LR : float = 2e-4\n",
    "    WEIGHT_DECAY : float = 0.0\n",
    "    OPTIMIZER_BETAS : List[float] = field(default_factory = lambda : [0.9, 0.999])\n",
    "    OPTIMIZER_EPS : float = 1e-8\n",
    "    OPTIMIZER_AMSGRAD : bool = False\n",
    "    GRADIENT_CLIP : float = None\n",
    "    SCHEDULER: str = None\n",
    "    SCHEDULER_T_MAX: int = None\n",
    "    SCHEDULER_ETA_MIN: float = None\n",
    "    SCHEDULER_GAMMA: float = None\n",
    "    SCHEDULER_STEP_SIZE: int = None\n",
    "\n",
    "    #...generation pipeline params:\n",
    "    SAMPLER : str = 'TauLeaping'\n",
    "    NUM_TIMESTEPS : int = 100\n",
    "    TIME_EPS : float = 1e-3\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.DATA = self.DATA_SOURCE + '_to_' + self.DATA_TARGET\n",
    "        self.DIM_INPUT = np.prod(self.INPUT_SHAPE)\n",
    "        if self.MULTI_GPU: self.DEVICE = 'cuda:0'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: building dataloaders...\n",
      "INFO: train/val/test split ratios: 0.83334/0.16667/0.0\n",
      "INFO: train size: 58333, validation size: 11666, testing sizes: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAA4CAAAAADLW8doAAABAElEQVR4nM2TS27FMAwDh0Huf+XpQpLtBmhWfcBzYMQO9aEoBVSgtgrrzmX6tFcaNwjBSNooAEiefkHSEFwaUvY5LOsjqvPNYgUoeu/8kcMIU8/ci52Zazr3cDks5SpqKz+oZAKARWS9a1ueg6WlI6xyu5JWli12Qa5ATp2gXME0w2Cznfhs2/EdMbwNux/l11K1FuNQnGpXs8srLcrRVFcNc9ok7JwTP1X+UqkTHpTZ3c1qEk6+nqJKXG3LOWRDEaaG8wDita3cxoVlpKyQmSJLoLtmJ44za5SnBx9fv3646wV7gnwZ6Bv4MUL5E3zw+b+ceQPfW/bW7Lfls5gvWz/W3em1F3XJHwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x56>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.transforms import ToPILImage\n",
    "from cmb.data.binary_mnist import MNISTDataClass\n",
    "from cmb.data.utils import DefineDataloader\n",
    "\n",
    "conf = Configs()\n",
    "mnist = MNISTDataClass(conf)\n",
    "dataloader = DefineDataloader(mnist)\n",
    "transform = ToPILImage()\n",
    "\n",
    "for batch in dataloader.train:\n",
    "    pair = torch.cat([batch.source[0].view(1, 28,28), batch.target[0].view( 1, 28,28)], dim=1)\n",
    "    img = transform(pair)\n",
    "    break\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-09 19:11:17.104309: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-09 19:11:17.149065: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-09 19:11:17.150003: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-09 19:11:17.882615: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "number of training parameters: 323872\n",
      "start training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: building dataloaders...\n",
      "INFO: train/val/test split ratios: 0.83334/0.16667/0.0\n",
      "INFO: train size: 58333, validation size: 11666, testing sizes: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfdb661bff324099aaca151fdffbc0c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cmb.dynamics.cjb import ConditionalJumpBridge\n",
    "from cmb.models.architectures.state_classifier import StateClassifier \n",
    "from cmb.models.trainers import CMBTrainer\n",
    "\n",
    "conf = Configs()\n",
    "mnist = MNISTDataClass(conf)\n",
    "dynamics = ConditionalJumpBridge(conf)\n",
    "classifier = StateClassifier(conf)\n",
    "generative_model = CMBTrainer(dynamics, classifier, mnist)\n",
    "generative_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "class TransitionRateModel(torch.nn.Module):\n",
    "    def __init__(self, model, config):\n",
    "        super().__init__()\n",
    "        self.model = model # model should output logits\n",
    "        self.vocab_size = config.VOCAB_SIZE\n",
    "        self.config = config\n",
    "        self.gamma = config.GAMMA\n",
    "        self.time_epsilon = config.TIME_EPS\n",
    "\n",
    "    def forward(self, t, s, context=None):\n",
    "        t = t.squeeze()\n",
    "        if len(s.shape) != 2:\n",
    "            s = s.reshape(s.size(0),-1)\n",
    "        logits = self.model(t, s, context)\n",
    "        t1 = 1. - self.time_epsilon\n",
    "        beta_integral = (t1 - t) * self.gamma\n",
    "        wt = torch.exp(-self.vocab_size * beta_integral)\n",
    "        A, B, C = 1. , (wt * self.vocab_size)/(1. - wt) , wt\n",
    "        qx = softmax(logits, dim=2)\n",
    "        qy = torch.gather(qx, 2, s.long().unsqueeze(2))\n",
    "        rate = A + B[:, None, None] * qx + C[:, None, None] * qy\n",
    "        return rate\n",
    "\n",
    "class TauLeapingSolver:\n",
    "    def __init__(self, transition_rate, config):\n",
    "        self.transition_rate = transition_rate\n",
    "        self.device = config.DEVICE\n",
    "        self.dim = config.DIM_INPUT\n",
    "        self.vocab_size = config.VOCAB_SIZE \n",
    "\n",
    "    def simulate(self, x, t_span, argmax=False):\n",
    "        time_steps = len(t_span)\n",
    "        tau = (t_span[-1] - t_span[0]) / (time_steps - 1)\n",
    "        trajectory = [x]\n",
    "\n",
    "        for i in range(1, time_steps):\n",
    "            t = t_span[i-1]\n",
    "    \n",
    "            current_state = x.clone()\n",
    "            rates = self.transition_rate(t, current_state).to(self.device)\n",
    "            max_rate = torch.max(rates, dim=2)[1]\n",
    "\n",
    "            jumps = torch.poisson(rates * tau).to(self.device) \n",
    "            mask =  torch.sum(jumps, dim=-1).type_as(current_state) <= 1\n",
    "            diff = torch.arange(self.vocab_size, device=self.device).view(1, 1, self.vocab_size) - x[:,:, None]\n",
    "            net_jumps = torch.sum(jumps * diff, dim=-1).type_as(current_state)\n",
    "            \n",
    "            x = current_state + net_jumps * mask\n",
    "            x = torch.clamp(x, min=0, max=self.vocab_size-1)            \n",
    "            trajectory.append(x.clone())\n",
    "\n",
    "        return torch.stack(trajectory), max_rate\n",
    "\n",
    "\n",
    "\n",
    "class ContextWrapper(torch.nn.Module):\n",
    "    \"\"\" Wraps model to torchdyn compatible format.\n",
    "    \"\"\"\n",
    "    def __init__(self, net, context=None):\n",
    "        super().__init__()\n",
    "        self.nn = net\n",
    "        self.context = context\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        t = t.repeat(x.shape[0])\n",
    "        t = self.reshape_time_like(t, x)\n",
    "        return self.nn(t=t, s=x, context=self.context)\n",
    "\n",
    "    def reshape_time_like(self, t, x):\n",
    "        if isinstance(t, (float, int)): return t\n",
    "        else: return t.reshape(-1, *([1] * (x.dim() - 1)))\n",
    "\n",
    "\n",
    "class CJBPipeline:\n",
    "    def __init__(self, \n",
    "                 trained_model, \n",
    "                 config: dataclass=None,\n",
    "                 best_epoch_model: bool=True\n",
    "                 ):\n",
    "\n",
    "        self.config = config\n",
    "        self.model = trained_model.best_epoch_model if best_epoch_model else trained_model.last_epoch_model\n",
    "        self.num_sampling_steps = config.NUM_TIMESTEPS\n",
    "        self.sampler = config.SAMPLER\n",
    "        self.device = config.DEVICE\n",
    "        self.vocab_size = config.VOCAB_SIZE\n",
    "        self.has_context = True if config.DIM_CONTEXT > 0 else False\n",
    "        self.time_steps = torch.linspace(0.0, 1.0 - config.TIME_EPS, self.num_sampling_steps, device=self.device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_samples(self, input_source, context=None):\n",
    "        self.source = input_source.to(self.device) \n",
    "        self.context = context.to(self.device) if self.has_context else None\n",
    "        jumps, x1 = self.MarkovSolver() \n",
    "        self.jumps = jumps.detach().cpu()\n",
    "        self.x1 = x1.detach().cpu()\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def MarkovSolver(self):\n",
    "        rate = TransitionRateModel(self.model, self.config)\n",
    "        rate = ContextWrapper(rate, context=self.context if self.context is not None else None)\n",
    "        tau_leaping = TauLeapingSolver(transition_rate=rate, config=self.config)\n",
    "        return tau_leaping.simulate(x=self.source, t_span=self.time_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-09 19:19:15.673776: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-09 19:19:15.717977: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-09 19:19:15.719114: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-09 19:19:16.508032: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: building dataloaders...\n",
      "INFO: train/val/test split ratios: 0.83334/0.16667/0.0\n",
      "INFO: train size: 58333, validation size: 11666, testing sizes: 0\n"
     ]
    }
   ],
   "source": [
    "from cmb.dynamics.cjb import ConditionalJumpBridge\n",
    "from cmb.models.architectures.state_classifier import StateClassifier \n",
    "from cmb.models.trainers import CMBTrainer\n",
    "from cmb.data.binary_mnist import MNISTDataClass\n",
    "\n",
    "conf = Configs()\n",
    "mnist = MNISTDataClass(conf)\n",
    "dynamics = ConditionalJumpBridge(conf)\n",
    "classifier = StateClassifier(conf)\n",
    "generative_model = CMBTrainer(dynamics, classifier, mnist)\n",
    "\n",
    "generative_model.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m pipeline \u001b[39m=\u001b[39m CJBPipeline(trained_model\u001b[39m=\u001b[39mgenerative_model, config\u001b[39m=\u001b[39mconf)\n\u001b[1;32m      9\u001b[0m input_source \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, (\u001b[39m128\u001b[39m, \u001b[39m784\u001b[39m))\n\u001b[0;32m---> 11\u001b[0m pipeline\u001b[39m.\u001b[39;49mgenerate_samples(input_source)\n\u001b[1;32m     12\u001b[0m sample \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39mjumps\u001b[39m.\u001b[39mview(N, \u001b[39m128\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m28\u001b[39m, \u001b[39m28\u001b[39m)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     13\u001b[0m x1 \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39mx1\u001b[39m.\u001b[39mview(\u001b[39m128\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m28\u001b[39m, \u001b[39m28\u001b[39m)\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/.conda/envs/flow_match_env/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/CMB/src/cmb/models/pipelines.py:74\u001b[0m, in \u001b[0;36mCJBPipeline.generate_samples\u001b[0;34m(self, input_source, context)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource \u001b[39m=\u001b[39m input_source\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \n\u001b[1;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_context \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m jumps, x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mMarkovSolver() \n\u001b[1;32m     75\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjumps \u001b[39m=\u001b[39m jumps\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m     76\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx1 \u001b[39m=\u001b[39m x1\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/.conda/envs/flow_match_env/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/CMB/src/cmb/models/pipelines.py:83\u001b[0m, in \u001b[0;36mCJBPipeline.MarkovSolver\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m rate \u001b[39m=\u001b[39m ContextWrapper(rate, context\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     82\u001b[0m tau_leaping \u001b[39m=\u001b[39m TauLeapingSolver(transition_rate\u001b[39m=\u001b[39mrate, config\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig)\n\u001b[0;32m---> 83\u001b[0m \u001b[39mreturn\u001b[39;00m tau_leaping\u001b[39m.\u001b[39;49msimulate(s\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource, t_span\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtime_steps)\n",
      "File \u001b[0;32m~/CMB/src/cmb/models/utils.py:278\u001b[0m, in \u001b[0;36mTauLeapingSolver.simulate\u001b[0;34m(self, s, t_span)\u001b[0m\n\u001b[1;32m    275\u001b[0m t \u001b[39m=\u001b[39m t_span[i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    277\u001b[0m current_state \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mclone()\n\u001b[0;32m--> 278\u001b[0m rates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransition_rate(t, s\u001b[39m=\u001b[39;49mcurrent_state, x\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    279\u001b[0m max_rate \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(rates, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m    281\u001b[0m jumps \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mpoisson(rates \u001b[39m*\u001b[39m tau)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \n",
      "File \u001b[0;32m~/.conda/envs/flow_match_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/CMB/src/cmb/models/utils.py:237\u001b[0m, in \u001b[0;36mContextWrapper.forward\u001b[0;34m(self, t, x, s)\u001b[0m\n\u001b[1;32m    235\u001b[0m     t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mrepeat(s\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m    236\u001b[0m     t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreshape_time_like(t, s)\n\u001b[0;32m--> 237\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnn(t\u001b[39m=\u001b[39;49mt, x\u001b[39m=\u001b[39;49mx, s\u001b[39m=\u001b[39;49ms, context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext)\n",
      "File \u001b[0;32m~/.conda/envs/flow_match_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'x'"
     ]
    }
   ],
   "source": [
    "from cmb.models.pipelines import CJBPipeline\n",
    "\n",
    "conf.GAMMA = 0.001\n",
    "conf.NUM_TIMESTEPS = N = int(1. / conf.GAMMA)\n",
    "conf.TIME_EPS = 0.0\n",
    "\n",
    "\n",
    "pipeline = CJBPipeline(trained_model=generative_model, config=conf)\n",
    "input_source = torch.randint(0, 2, (128, 784))\n",
    "\n",
    "pipeline.generate_samples(input_source)\n",
    "sample = pipeline.jumps.view(N, 128, 1, 28, 28).float()\n",
    "x1 = pipeline.x1.view(128, 1, 28, 28).float()\n",
    "\n",
    "print('number of timesteps: {}'.format(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=336x280>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "transform = ToPILImage()\n",
    "generated_sample = []\n",
    "\n",
    "boundary = torch\n",
    "\n",
    "for idx in random.sample(range(0, 128), 10):\n",
    "    img = []\n",
    "    img1 = []\n",
    "    \n",
    "    for time_step in range(0, N, N//10):\n",
    "        img.append(sample[time_step, idx,:,:,:]) \n",
    "    img.append(sample[-1, idx,:,:,:]) \n",
    "    img.append(x1[idx,:,:,:])\n",
    "    img = torch.cat(img, dim=-1)\n",
    "    generated_sample.append(img)\n",
    "\n",
    "generated_sample = torch.cat(generated_sample, dim=1)\n",
    "transform(generated_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow_match_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
